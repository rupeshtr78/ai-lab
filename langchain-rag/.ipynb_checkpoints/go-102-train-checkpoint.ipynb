{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from git import Repo\n",
    "from langchain_community.document_loaders.generic import GenericLoader\n",
    "from langchain_community.document_loaders.parsers import LanguageParser\n",
    "from langchain_text_splitters import Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Clone\n",
    "repo_path = \"/home/rupesh/aqrtr/ai/langchain/proj-101-go/go_repo-01\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo = Repo.clone_from(\"https://github.com/rupeshtr78/nvidia-server\", to_path=repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load\n",
    "loader = GenericLoader.from_filesystem(\n",
    "    repo_path,\n",
    "    glob=\"**/*\",\n",
    "    suffixes=[\".go\"],\n",
    "    exclude=[\"Dockerfile\", \"vendor\", \"docker-compose.yml\", \"Makefile\", \"README.md\"],\n",
    "    parser=LanguageParser(language=Language.GO, parser_threshold=500),\n",
    ")\n",
    "documents = loader.load()\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': '/home/rupesh/aqrtr/ai/langchain/proj-101-go/go_repo-01/internal/metrics/fetch_gpu_metrics.go', 'language': <Language.GO: 'go'>}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'source': '/home/rupesh/aqrtr/ai/langchain/proj-101-go/go_repo-01/internal/metrics/gpu_device.go', 'language': <Language.GO: 'go'>}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'source': '/home/rupesh/aqrtr/ai/langchain/proj-101-go/go_repo-01/internal/metrics/gpu_metrics.go', 'language': <Language.GO: 'go'>}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'source': '/home/rupesh/aqrtr/ai/langchain/proj-101-go/go_repo-01/internal/metrics/gpu_device_test.go', 'language': <Language.GO: 'go'>}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'source': '/home/rupesh/aqrtr/ai/langchain/proj-101-go/go_repo-01/internal/metrics/fetch_gpu_mertics_test.go', 'language': <Language.GO: 'go'>}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'source': '/home/rupesh/aqrtr/ai/langchain/proj-101-go/go_repo-01/internal/metrics/fetch_gpu_metrics_v2_test.go', 'language': <Language.GO: 'go'>}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'source': '/home/rupesh/aqrtr/ai/langchain/proj-101-go/go_repo-01/internal/metrics/gpu_metrics_test.go', 'language': <Language.GO: 'go'>}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'source': '/home/rupesh/aqrtr/ai/langchain/proj-101-go/go_repo-01/server/server_test.go', 'language': <Language.GO: 'go'>}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'source': '/home/rupesh/aqrtr/ai/langchain/proj-101-go/go_repo-01/server/server.go', 'language': <Language.GO: 'go'>}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'source': '/home/rupesh/aqrtr/ai/langchain/proj-101-go/go_repo-01/cmd/main.go', 'language': <Language.GO: 'go'>}\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for doc in documents:\n",
    "    print(doc.metadata)\n",
    "    print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "go_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.GO, chunk_size=2000, chunk_overlap=200\n",
    ")\n",
    "texts = go_splitter.split_documents(documents)\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='package gpumetrics\\n\\nimport (\\n\\t\"context\"\\n\\t\"fmt\"\\n\\t\"log\"\\n\\t\"sync\"\\n\\n\\t\"github.com/NVIDIA/go-nvml/pkg/nvml\"\\n)\\n\\n// FetchGpuInfo fetches the metrics for all GPU devices\\nfunc FetchAllGpuInfo(ctx context.Context, gpu GpuDeviceManager, count int) (GpuMap, error) {\\n\\n\\tif count == 0 {\\n\\t\\treturn nil, fmt.Errorf(\"no GPU devices found\")\\n\\t}\\n\\n\\tgpuMap := make(GpuMap)\\n\\n\\t// gpuChan := make(chan GpuMap, count)\\n\\t// defer close(gpuChan)\\n\\n\\terrChan := make(chan error, 1) // only need to store one error\\n\\tdefer close(errChan)\\n\\n\\twg := new(sync.WaitGroup)\\n\\twg.Add(count)\\n\\n\\tfor i := 0; i < count; i++ {\\n\\t\\tgo func(i int) {\\n\\t\\t\\tdefer wg.Done()\\n\\n\\t\\t\\t// context done means there has been a cancellation signal\\n\\t\\t\\tselect {\\n\\t\\t\\tcase <-ctx.Done():\\n\\t\\t\\t\\terrChan <- ctx.Err()\\n\\t\\t\\t\\treturn\\n\\t\\t\\tdefault:\\n\\t\\t\\t}\\n\\n\\t\\t\\tdevice, ret := gpu.DeviceGetHandleByIndex(i)\\n\\t\\t\\tif ret != nvml.SUCCESS {\\n\\t\\t\\t\\terrChan <- fmt.Errorf(\"failed to get device handle: %v\", ret)\\n\\t\\t\\t\\treturn\\n\\t\\t\\t}\\n\\n\\t\\t\\tg, err := FetchDeviceMetrics(device)\\n\\t\\t\\tif err != nvml.SUCCESS {\\n\\t\\t\\t\\terrChan <- fmt.Errorf(\"failed to get device info: %v\", err)\\n\\t\\t\\t\\treturn\\n\\t\\t\\t}\\n\\n\\t\\t\\tgpuMap[i] = g\\n\\t\\t}(i)\\n\\t}\\n\\n\\t// main goroutine waits for all goroutines to finish\\n\\twg.Wait()\\n\\n\\tfor i := 0; i < count; i++ {\\n\\t\\tselect {\\n\\t\\tcase <-ctx.Done(): // if context is done, ( cancelled or timeout ) return ctx.err\\n\\t\\t\\treturn nil, ctx.Err()\\n\\t\\tcase err := <-errChan: // if there is an error, return err\\n\\t\\t\\treturn nil, err\\n\\t\\tdefault:\\n\\t\\t\\tlog.Printf(\"Fetched Metrics for GPU %d\\\\n\", i)\\n\\t\\t}\\n\\t}\\n\\n\\treturn gpuMap, nil\\n}', metadata={'source': '/home/rupesh/aqrtr/ai/langchain/proj-101-go/go_repo-01/internal/metrics/fetch_gpu_metrics.go', 'language': <Language.GO: 'go'>})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "db = Chroma.from_documents(texts, OpenAIEmbeddings(disallowed_special=()))\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"mmr\",  # Also test \"similarity\"\n",
    "    search_kwargs={\"k\": 8},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4\")\n",
    "memory = ConversationSummaryMemory(llm=llm, max_memory_length=5, max_memory_turns=5, return_messages=True, memory_key=\"chat_history\")\n",
    "qaChain = ConversationalRetrievalChain.from_llm(llm=llm, memory=memory, retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 20 is greater than number of elements in index 18, updating n_results = 18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Error Handling: The `FetchAllGpuInfo` function could use more comprehensive error handling. Right now, it only returns the first error it encounters, but it could be more informative if it collected all errors and returned them together.\n",
      "\n",
      "2. Parallelism Control: The function spawns a goroutine for each GPU device, which could potentially lead to issues if the count of GPU devices is large. It could be improved by using a worker pool pattern to limit the number of concurrent goroutines.\n",
      "\n",
      "3. Context Usage: The function could make better use of the context passed in. It could pass the context to the `FetchDeviceMetrics` function to allow for cancellation of that function as well.\n",
      "\n",
      "4. Logging: The function could use structured logging to make the logs easier to parse and analyze. Additionally, it could include more information in the logs, such as the specific GPU device that an operation is being performed on.\n",
      "\n",
      "5. Code Duplication: The function has some duplicated code that could be refactored. For example, the error handling and logging code in the goroutines are essentially the same and could be moved into a separate function.\n",
      "\n",
      "6. Result Ordering: The function could ensure that the results are in the same order as the devices. Right now, due to the concurrent nature of the function, the order of the results in the `gpuMap` may not match the order of the devices.\n",
      "\n",
      "7. Testing: The function could be made more testable by breaking it down into smaller, more isolated functions, each with a single responsibility. This would make it easier to write unit tests for each individual component of the function.\n"
     ]
    }
   ],
   "source": [
    "# Ask a question\n",
    "question = \"Can you suggest some improvements for function FetchAllGpuInfo?\"  # Replace with your actual question\n",
    "\n",
    "# Include 'chat_history' key if required by your function\n",
    "formatted_question = {'question': question, 'chat_history': ''}\n",
    "\n",
    "# Invoke the conversation chain with the properly formatted question\n",
    "result = qaChain.invoke(formatted_question)\n",
    "\n",
    "# Fetch and print the answer\n",
    "answer = result['answer']\n",
    "print(answer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_community.chat_models.ollama import ChatOllama\n",
    "llmOllama = ChatOllama(model=\"llama3:70b\")\n",
    "memoryOllama = ConversationSummaryMemory(llm=llmOllama, max_memory_length=5, max_memory_turns=5, return_messages=True, memory_key=\"chat_history\")\n",
    "ollamaChain = ConversationalRetrievalChain.from_llm(llm=llmOllama, memory=memoryOllama, retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 20 is greater than number of elements in index 18, updating n_results = 18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing tests for the `FetchAllGpuInfo` function involves creating a test scenario where you can control the inputs and expected outputs. Here's an example of how you can approach this:\n",
      "\n",
      "**Test Strategy**\n",
      "\n",
      "1. Create a mock implementation of the `GpuDeviceManager` interface, which will allow you to control the behavior of the `DeviceGetHandleByIndex`, `GetUUID`, `GetName`, `GetTemperature`, `GetPowerUsage`, and `GetMemoryInfo` methods.\n",
      "2. Write test cases for different scenarios:\n",
      "\t* Happy path: all GPU info is fetched successfully\n",
      "\t* Error handling: one or more GPU info fetching fails\n",
      "\t* Edge cases: empty list of GPUs, invalid GPU indices, etc.\n",
      "\n",
      "**Example Test Code**\n",
      "\n",
      "Here's an example of how you can write tests for the `FetchAllGpuInfo` function:\n",
      "```go\n",
      "package gpumetrics\n",
      "\n",
      "import (\n",
      "\t\"context\"\n",
      "\t\"testing\"\n",
      "\n",
      "\t\"github.com/stretchr/testify/assert\"\n",
      ")\n",
      "\n",
      "func TestFetchAllGpuInfo(t *testing.T) {\n",
      "\tmockDeviceManager := new(MockGpuDeviceManager)\n",
      "\n",
      "\t// Happy path test\n",
      "\tt.Run(\"HappyPath\", func(t *testing.T) {\n",
      "\t\tmockDeviceManager.On(\"DeviceGetHandleByIndex\").Return(&MockNvmlDevice{}, nvml.SUCCESS)\n",
      "\t\tmockDeviceManager.On(\"GetUUID\").Return(\"mock-uuid\", nvml.SUCCESS)\n",
      "\t\tmockDeviceManager.On(\"GetName\").Return(\"mock-name\", nvml.SUCCESS)\n",
      "\t\tmockDeviceManager.On(\"GetTemperature\").Return(42, nvml.SUCCESS)\n",
      "\t\tmockDeviceManager.On(\"GetPowerUsage\").Return(100, nvml.SUCCESS)\n",
      "\t\tmockDeviceManager.On(\"GetMemoryInfo\").Return(nvml.Memory{Total: 1024}, nvml.SUCCESS)\n",
      "\n",
      "\t\tgpuCount := 1\n",
      "\t\tinfo, err := FetchAllGpuInfo(context.Background(), mockDeviceManager, gpuCount)\n",
      "\t\tassert.NoError(t, err)\n",
      "\t\tassert.Len(t, info, gpuCount)\n",
      "\t})\n",
      "\n",
      "\t// Error handling test\n",
      "\tt.Run(\"ErrorHandling\", func(t *testing.T) {\n",
      "\t\tmockDeviceManager.On(\"DeviceGetHandleByIndex\").Return(nil, nvml.ERROR_UNKNOWN)\n",
      "\t\tgpuCount := 1\n",
      "\t\tinfo, err := FetchAllGpuInfo(context.Background(), mockDeviceManager, gpuCount)\n",
      "\t\tassert.Error(t, err)\n",
      "\t})\n",
      "\n",
      "\t// Edge case test: empty list of GPUs\n",
      "\tt.Run(\"EmptyList\", func(t *testing.T) {\n",
      "\t\tmockDeviceManager.On(\"DeviceGetHandleByIndex\").Return(nil, nvml.ERROR_UNKNOWN)\n",
      "\t\tgpuCount := 0\n",
      "\t\tinfo, err := FetchAllGpuInfo(context.Background(), mockDeviceManager, gpuCount)\n",
      "\t\tassert.NoError(t, err)\n",
      "\t\tassert.Len(t, info, 0)\n",
      "\t})\n",
      "}\n",
      "```\n",
      "In this example, we create a `MockGpuDeviceManager` struct that implements the `GpuDeviceManager` interface. We use the `On` method to set up the expected behavior of each method.\n",
      "\n",
      "We then write three test cases:\n",
      "\n",
      "1. Happy path: all GPU info is fetched successfully.\n",
      "2. Error handling: one or more GPU info fetching fails.\n",
      "3. Edge case: empty list of GPUs.\n",
      "\n",
      "In each test case, we assert that the `FetchAllGpuInfo` function returns the expected output and error values.\n",
      "\n",
      "Note that you may need to add more test cases depending on your specific requirements.\n"
     ]
    }
   ],
   "source": [
    "# Ask a question\n",
    "question = \"Can you write tests for function FetchAllGpuInfo?\"  # Replace with your actual question\n",
    "\n",
    "# Include 'chat_history' key if required by your function\n",
    "formatted_question = {'question': question, 'chat_history': ''}\n",
    "\n",
    "# Invoke the conversation chain with the properly formatted question\n",
    "result = ollamaChain.invoke(formatted_question)\n",
    "\n",
    "# Fetch and print the answer\n",
    "answer = result['answer']\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 20 is greater than number of elements in index 18, updating n_results = 18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To implement a `MockGpuDeviceManager` for testing purposes, you can create a struct that satisfies the `GpuDeviceManager` interface and provides mock implementations for its methods.\n",
      "\n",
      "Here's an example implementation:\n",
      "```go\n",
      "type MockGpuDeviceManager struct {\n",
      "    // Mocked method return values\n",
      "    DeviceGetCountReturn  int\n",
      "    DeviceGetCountErr     nvml.Return\n",
      "    DeviceGetHandleByIndexReturn   *nvml.Device\n",
      "    DeviceGetHandleByIndexErr      nvml.Return\n",
      "    // ... add more mocked method return values as needed\n",
      "\n",
      "    // Call counters for testing purposes\n",
      "    DeviceGetCountCallCount  int\n",
      "    DeviceGetHandleByIndexCallCount int\n",
      "    // ... add more call counters as needed\n",
      "}\n",
      "\n",
      "func (m *MockGpuDeviceManager) DeviceGetCount() (int, nvml.Return) {\n",
      "    m.DeviceGetCountCallCount++\n",
      "    return m.DeviceGetCountReturn, m.DeviceGetCountErr\n",
      "}\n",
      "\n",
      "func (m *MockGpuDeviceManager) DeviceGetHandleByIndex(index int) (*nvml.Device, nvml.Return) {\n",
      "    m.DeviceGetHandleByIndexCallCount++\n",
      "    return m.DeviceGetHandleByIndexReturn, m.DeviceGetHandleByIndexErr\n",
      "}\n",
      "\n",
      "// ... implement more mocked methods as needed\n",
      "\n",
      "func (m *MockGpuDeviceManager) Reset() {\n",
      "    // Reset call counters and return values\n",
      "    m.DeviceGetCountCallCount = 0\n",
      "    m.DeviceGetHandleByIndexCallCount = 0\n",
      "    m.DeviceGetCountReturn = 0\n",
      "    m.DeviceGetCountErr = nvml.SUCCESS\n",
      "    m.DeviceGetHandleByIndexReturn = nil\n",
      "    m.DeviceGetHandleByIndexErr = nvml.SUCCESS\n",
      "}\n",
      "```\n",
      "In this example, the `MockGpuDeviceManager` struct has fields to store the mocked return values and call counters for each method. The methods themselves simply increment the call counter and return the pre-set values.\n",
      "\n",
      "You can use this mock implementation in your tests to control the behavior of the `GpuDeviceManager` interface. For example:\n",
      "```go\n",
      "func TestFetchAllGpuInfo_ContextCancellationV2(t *testing.T) {\n",
      "    // Create a mock GpuDeviceManager\n",
      "    gpu := &MockGpuDeviceManager{}\n",
      "    gpu.DeviceGetCountReturn = 2\n",
      "    gpu.DeviceGetCountErr = nvml.SUCCESS\n",
      "\n",
      "    // ... use the mock gpu manager in your test\n",
      "}\n",
      "```\n",
      "By using this mock implementation, you can isolate the dependencies of your code and focus on testing specific logic without relying on external systems.\n"
     ]
    }
   ],
   "source": [
    "# Ask a question\n",
    "question = \"Can you write MockGpuDeviceManager?\"  # Replace with your actual question\n",
    "\n",
    "# Include 'chat_history' key if required by your function\n",
    "formatted_question = {'question': question, 'chat_history': ''}\n",
    "\n",
    "# Invoke the conversation chain with the properly formatted question\n",
    "result = ollamaChain.invoke(formatted_question)\n",
    "\n",
    "# Fetch and print the answer\n",
    "answer = result['answer']\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 20 is greater than number of elements in index 18, updating n_results = 18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To implement a `MockNvmlDevice` in Go, you can use the `mock` package from the `github.com/stretchr/testify/mock` library. This package provides a way to create mock objects that satisfy interfaces.\n",
      "\n",
      "Here's an example implementation of a `MockNvmlDevice`:\n",
      "```go\n",
      "import (\n",
      "\t\"github.com/stretchr/testify/mock\"\n",
      "\t\"gitlab.com/nvidia/go-nvml/pkg/nvml\"\n",
      ")\n",
      "\n",
      "type MockNvmlDevice struct {\n",
      "\tmock.Mock\n",
      "}\n",
      "\n",
      "func (m *MockNvmlDevice) DeviceGetCount() (int, nvml.Return) {\n",
      "\targs := m.Called()\n",
      "\treturn args.Get(0).(int), args.Get(1).(nvml.Return)\n",
      "}\n",
      "\n",
      "func (m *MockNvmlDevice) DeviceGetHandleByIndex(index int) (*nvml.Device, nvml.Return) {\n",
      "\targs := m.Called(index)\n",
      "\treturn args.Get(0).(*nvml.Device), args.Get(1).(nvml.Return)\n",
      "}\n",
      "\n",
      "func (m *MockNvmlDevice) FetchDeviceMetrics(device *nvml.Device) (*NvidiaDevice, nvml.Return) {\n",
      "\targs := m.Called(device)\n",
      "\treturn args.Get(0).(*NvidiaDevice), args.Get(1).(nvml.Return)\n",
      "}\n",
      "```\n",
      "In this implementation, we define a `MockNvmlDevice` struct that embeds the `mock.Mock` type from the `github.com/stretchr/testify/mock` library. This allows us to use the `On` method to set up expectations for the mock object.\n",
      "\n",
      "We then implement the methods of the `nvml.Device` interface, such as `DeviceGetCount`, `DeviceGetHandleByIndex`, and `FetchDeviceMetrics`. Each method uses the `Callee\n"
     ]
    }
   ],
   "source": [
    "# Ask a question\n",
    "question = \"Can you write MockNvmlDevice?\"  # Replace with your actual question\n",
    "\n",
    "# Include 'chat_history' key if required by your function\n",
    "formatted_question = {'question': question, 'chat_history': ''}\n",
    "\n",
    "# Invoke the conversation chain with the properly formatted question\n",
    "result = ollamaChain.invoke(formatted_question)\n",
    "\n",
    "# Fetch and print the answer\n",
    "answer = result['answer']\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
