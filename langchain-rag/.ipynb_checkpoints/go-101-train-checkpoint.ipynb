{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from git import Repo\n",
    "from langchain_community.document_loaders.generic import GenericLoader\n",
    "from langchain_community.document_loaders.parsers import LanguageParser\n",
    "from langchain_text_splitters import Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone\n",
    "repo_path = \"/home/rupesh/aqrtr/ai/langchain/proj-101-go/go_repo-01\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo = Repo.clone_from(\"https://github.com/rupeshtr78/nvidia-server\", to_path=repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load\n",
    "loader = GenericLoader.from_filesystem(\n",
    "    repo_path,\n",
    "    glob=\"**/*\",\n",
    "    suffixes=[\".go\"],\n",
    "    exclude=[\"Dockerfile\", \"vendor\", \"docker-compose.yml\", \"Makefile\", \"README.md\"],\n",
    "    parser=LanguageParser(language=Language.GO, parser_threshold=500),\n",
    ")\n",
    "documents = loader.load()\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in documents:\n",
    "    print(doc.metadata)\n",
    "    print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "go_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.GO, chunk_size=2000, chunk_overlap=200\n",
    ")\n",
    "texts = go_splitter.split_documents(documents)\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='package gpumetrics\\n\\nimport (\\n\\t\"context\"\\n\\t\"fmt\"\\n\\t\"log\"\\n\\t\"sync\"\\n\\n\\t\"github.com/NVIDIA/go-nvml/pkg/nvml\"\\n)\\n\\n// FetchGpuInfo fetches the metrics for all GPU devices\\nfunc FetchAllGpuInfo(ctx context.Context, gpu GpuDeviceManager, count int) (GpuMap, error) {\\n\\n\\tif count == 0 {\\n\\t\\treturn nil, fmt.Errorf(\"no GPU devices found\")\\n\\t}\\n\\n\\tgpuMap := make(GpuMap)\\n\\n\\t// gpuChan := make(chan GpuMap, count)\\n\\t// defer close(gpuChan)\\n\\n\\terrChan := make(chan error, 1) // only need to store one error\\n\\tdefer close(errChan)\\n\\n\\twg := new(sync.WaitGroup)\\n\\twg.Add(count)\\n\\n\\tfor i := 0; i < count; i++ {\\n\\t\\tgo func(i int) {\\n\\t\\t\\tdefer wg.Done()\\n\\n\\t\\t\\t// context done means there has been a cancellation signal\\n\\t\\t\\tselect {\\n\\t\\t\\tcase <-ctx.Done():\\n\\t\\t\\t\\terrChan <- ctx.Err()\\n\\t\\t\\t\\treturn\\n\\t\\t\\tdefault:\\n\\t\\t\\t}\\n\\n\\t\\t\\tdevice, ret := gpu.DeviceGetHandleByIndex(i)\\n\\t\\t\\tif ret != nvml.SUCCESS {\\n\\t\\t\\t\\terrChan <- fmt.Errorf(\"failed to get device handle: %v\", ret)\\n\\t\\t\\t\\treturn\\n\\t\\t\\t}\\n\\n\\t\\t\\tg, err := FetchDeviceMetrics(device)\\n\\t\\t\\tif err != nvml.SUCCESS {\\n\\t\\t\\t\\terrChan <- fmt.Errorf(\"failed to get device info: %v\", err)\\n\\t\\t\\t\\treturn\\n\\t\\t\\t}\\n\\n\\t\\t\\tgpuMap[i] = g\\n\\t\\t}(i)\\n\\t}\\n\\n\\t// main goroutine waits for all goroutines to finish\\n\\twg.Wait()\\n\\n\\tfor i := 0; i < count; i++ {\\n\\t\\tselect {\\n\\t\\tcase <-ctx.Done(): // if context is done, ( cancelled or timeout ) return ctx.err\\n\\t\\t\\treturn nil, ctx.Err()\\n\\t\\tcase err := <-errChan: // if there is an error, return err\\n\\t\\t\\treturn nil, err\\n\\t\\tdefault:\\n\\t\\t\\tlog.Printf(\"Fetched Metrics for GPU %d\\\\n\", i)\\n\\t\\t}\\n\\t}\\n\\n\\treturn gpuMap, nil\\n}', metadata={'source': '/home/rupesh/aqrtr/ai/langchain/proj-101-go/go_repo-01/internal/metrics/fetch_gpu_metrics.go', 'language': <Language.GO: 'go'>})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "db = Chroma.from_documents(texts, OpenAIEmbeddings(disallowed_special=()))\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"mmr\",  # Also test \"similarity\"\n",
    "    search_kwargs={\"k\": 8},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4\")\n",
    "\n",
    "# First we need a prompt that we can pass into an LLM to generate this search query\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "        (\n",
    "            \"user\",\n",
    "            \"Given the above conversation, generate a search query to look up to get information relevant to the conversation\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "retriever_chain = create_history_aware_retriever(llm, retriever, prompt)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Answer the user's questions based on the below context:\\n\\n{context}\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "qa = create_retrieval_chain(retriever_chain, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 20 is greater than number of elements in index 18, updating n_results = 18\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'FetchAllGpuInfo is a function in the gpumetrics package. This function fetches metrics for all GPU devices. It takes in a context, a GPU device manager, and a count representing the number of GPU devices. It returns a GpuMap and an error (if any). \\n\\nThe function creates a goroutine for each GPU device to fetch its metrics concurrently. If any error occurs during the fetching of metrics for a device, the error is sent to an error channel. After all goroutines have finished their work, the function checks for any errors that might have occurred and logs the successful fetching of metrics. \\n\\nIf the context is cancelled or times out, the function will stop fetching metrics and return the cancellation or timeout error.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What is a FetchAllGpuInfo?\"\n",
    "result = qa.invoke({\"input\": question})\n",
    "result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 20 is greater than number of elements in index 18, updating n_results = 18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> **Question**: What one improvement do you propose in code in relation go code? \n",
      "\n",
      "**Answer**: The code appears well-structured and follows good software engineering practices. However, one area of improvement could be the error handling. Currently in the FetchAllGpuInfo function, when an error occurs in the goroutine, the error is written to the errChan channel and the function returns. However, the main goroutine waits for all goroutines to finish and then checks for errors. If an error occurs during the execution of one goroutine, the other goroutines will still continue executing, which could potentially waste resources if the error is critical and means the other goroutines should not continue. A possible improvement could be to add a mechanism to stop all running goroutines when an error occurs. The sync package's ErrGroup or a context cancellation could be used for this purpose. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"What one improvement do you propose in code in relation go code?\",\n",
    "    \"Can you complete the test case for the function FetchAllGpuInfo?\",\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    result = qa.invoke({\"input\": question})\n",
    "    print(f\"-> **Question**: {question} \\n\")\n",
    "    print(f\"**Answer**: {result['answer']} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 20 is greater than number of elements in index 18, updating n_results = 18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> **Question**: Can you complete the test cases for the function FetchAllGpuInfo? \n",
      "\n",
      "**Answer**: Based on the given context, I can provide a high-level description of how you might approach creating test cases for the FetchAllGpuInfo function. However, please note that the specifics will depend on your particular application and setup.\n",
      "\n",
      "Here is how you might go about it:\n",
      "\n",
      "1. Test case when `count` is 0:\n",
      "   - This test case should check if the function returns an error when it's called with a count of 0. The GPU device manager mock should be set to return 0 devices.\n",
      "\n",
      "2. Test case when all devices are fetched successfully:\n",
      "   - This test case should check if the function fetches all GPU device info correctly. The mock should be set to return a certain number of devices, and the function should be called with the same count. The test should assert that the returned GpuMap has the correct length and that no errors were returned.\n",
      "\n",
      "3. Test case when context is cancelled:\n",
      "   - This test case should check if the function handles a context cancellation correctly. The context should be cancelled before calling the function, and the test should assert that the function returns a context cancellation error.\n",
      "\n",
      "4. Test case when there is an error fetching device metrics:\n",
      "   - This test case should check if the function handles an error while fetching device metrics. The mock should be set to return an error when `FetchDeviceMetrics` is called. The test should assert that the function returns the same error.\n",
      "\n",
      "5. Test case when there is an error getting a device handle:\n",
      "   - This test case should check if the function handles an error when getting a device handle. The mock should be set to return an error when `DeviceGetHandleByIndex` is called. The test should assert that the function returns the same error.\n",
      "\n",
      "Please note that since the actual implementation of the FetchAllGpuInfo function involves goroutines and channel operations, these test cases might involve the use of additional synchronization primitives or techniques to accurately capture and test the function's behavior. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"Can you complete the test cases for the function FetchAllGpuInfo?\",\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    result = qa.invoke({\"input\": question})\n",
    "    print(f\"-> **Question**: {question} \\n\")\n",
    "    print(f\"**Answer**: {result['answer']} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4\")\n",
    "memory = ConversationSummaryMemory(llm=llm, max_memory_length=5, max_memory_turns=5, return_messages=True, memory_key=\"chat_history\")\n",
    "qaChain = ConversationalRetrievalChain.from_llm(llm=llm, memory=memory, retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's a basic example of a test for the FetchAllGpuInfo function. This test assumes that you have a mock implementation of GpuDeviceManager that can be used to simulate different conditions.\n",
      "\n",
      "```go\n",
      "package gpumetrics\n",
      "\n",
      "import (\n",
      "\t\"context\"\n",
      "\t\"testing\"\n",
      "\t\"time\"\n",
      "\n",
      "\t\"github.com/NVIDIA/go-nvml/pkg/nvml\"\n",
      "\t\"github.com/stretchr/testify/assert\"\n",
      ")\n",
      "\n",
      "func TestFetchAllGpuInfo(t *testing.T) {\n",
      "\t// Create a mock GpuDeviceManager\n",
      "\tmockDevice := new(MockGpuDeviceManager)\n",
      "\n",
      "\t// Set up the mock to return 2 devices\n",
      "\tmockDevice.On(\"DeviceGetCount\").Return(2, nvml.SUCCESS)\n",
      "\n",
      "\t// Set up the mock to return device handles for each index\n",
      "\tmockDevice.On(\"DeviceGetHandleByIndex\", 0).Return(&nvml.Device{}, nvml.SUCCESS)\n",
      "\tmockDevice.On(\"DeviceGetHandleByIndex\", 1).Return(&nvml.Device{}, nvml.SUCCESS)\n",
      "\n",
      "\t// Set up the mock to return metrics for each device\n",
      "\tmockDevice.On(\"FetchDeviceMetrics\", &nvml.Device{}).Return(&NvidiaDevice{}, nvml.SUCCESS)\n",
      "\tmockDevice.On(\"FetchDeviceMetrics\", &nvml.Device{}).Return(&NvidiaDevice{}, nvml.SUCCESS)\n",
      "\n",
      "\t// Create a context\n",
      "\tctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)\n",
      "\tdefer cancel()\n",
      "\n",
      "\t// Call the function under test\n",
      "\tgpuMap, err := FetchAllGpuInfo(ctx, mockDevice, 2)\n",
      "\n",
      "\t// Assert the result\n",
      "\tassert.NoError(t, err)\n",
      "\tassert.Equal(t, 2, len(gpuMap))\n",
      "\n",
      "\tmockDevice.AssertExpectations(t)\n",
      "}\n",
      "```\n",
      "\n",
      "This test creates a mock GpuDeviceManager and sets up the mock to return specific values when its methods are called. Then it calls the FetchAllGpuInfo function and checks that the function returns the expected results.\n"
     ]
    }
   ],
   "source": [
    "# Ask a question\n",
    "question = \"Write Test for FetchAllGpuInfo?\"  # Replace with your actual question\n",
    "\n",
    "# Include 'chat_history' key if required by your function\n",
    "formatted_question = {'question': question, 'chat_history': ''}\n",
    "\n",
    "# Invoke the conversation chain with the properly formatted question\n",
    "result = qaChain.invoke(formatted_question)\n",
    "\n",
    "# Fetch and print the answer\n",
    "answer = result['answer']\n",
    "print(answer)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
