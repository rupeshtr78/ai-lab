{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from git import Repo\n",
    "from langchain_community.document_loaders.generic import GenericLoader\n",
    "from langchain_community.document_loaders.parsers import LanguageParser\n",
    "from langchain_text_splitters import Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone\n",
    "repo_path = \"/home/rupesh/aqrtr/ai/langchain/proj-101-go/lc_repo\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo = Repo.clone_from(\"https://github.com/langchain-ai/langchain\", to_path=repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "349"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load\n",
    "loader = GenericLoader.from_filesystem(\n",
    "    repo_path + \"/libs/core/langchain_core\",\n",
    "    glob=\"**/*\",\n",
    "    suffixes=[\".py\"],\n",
    "    exclude=[\"**/non-utf8-encoding.py\"],\n",
    "    parser=LanguageParser(language=Language.PYTHON, parser_threshold=500),\n",
    ")\n",
    "documents = loader.load()\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1050"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON, chunk_size=2000, chunk_overlap=200\n",
    ")\n",
    "texts = python_splitter.split_documents(documents)\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "db = Chroma.from_documents(texts, OpenAIEmbeddings(disallowed_special=()))\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"mmr\",  # Also test \"similarity\"\n",
    "    search_kwargs={\"k\": 8},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4\")\n",
    "\n",
    "# First we need a prompt that we can pass into an LLM to generate this search query\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "        (\n",
    "            \"user\",\n",
    "            \"Given the above conversation, generate a search query to look up to get information relevant to the conversation\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "retriever_chain = create_history_aware_retriever(llm, retriever, prompt)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Answer the user's questions based on the below context:\\n\\n{context}\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "qa = create_retrieval_chain(retriever_chain, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A `RunnableBinding` is a class in the LangChain library that is used to bind a runnable (an instance of a class that subclasses the base `Runnable` class) and its parameters. It is a base class that has methods for backward propagation (`backward`) and updating parameters (`update`) with an optimizer. The binding allows parameters to be updated during the execution of the runnable. This is especially useful in machine learning applications where parameters of a model need to be updated iteratively based on the gradient of the loss function.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What is a RunnableBinding?\"\n",
    "result = qa.invoke({\"input\": question})\n",
    "result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"What classes are derived from the Runnable class?\",\n",
    "    \"What one improvement do you propose in code in relation to the class hierarchy for the Runnable class?\",\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    result = qa.invoke({\"input\": question})\n",
    "    print(f\"-> **Question**: {question} \\n\")\n",
    "    print(f\"**Answer**: {result['answer']} \\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
