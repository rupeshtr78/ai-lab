{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from git import Repo\n",
    "from langchain_community.document_loaders.generic import GenericLoader\n",
    "from langchain_community.document_loaders.parsers import LanguageParser\n",
    "from langchain_text_splitters import Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Clone\n",
    "repo_path = \"/home/rupesh/aqrtr/ai/langchain/proj-101-go/go_repo-01\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo = Repo.clone_from(\"https://github.com/rupeshtr78/nvidia-server\", to_path=repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load\n",
    "loader = GenericLoader.from_filesystem(\n",
    "    repo_path,\n",
    "    glob=\"**/*\",\n",
    "    suffixes=[\".go\"],\n",
    "    exclude=[\"Dockerfile\", \"vendor\", \"docker-compose.yml\", \"Makefile\", \"README.md\"],\n",
    "    parser=LanguageParser(language=Language.GO, parser_threshold=500),\n",
    ")\n",
    "documents = loader.load()\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for doc in documents:\n",
    "    print(doc.metadata)\n",
    "    print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "go_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.GO, chunk_size=2000, chunk_overlap=200\n",
    ")\n",
    "texts = go_splitter.split_documents(documents)\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='package gpumetrics\\n\\nimport (\\n\\t\"context\"\\n\\t\"fmt\"\\n\\t\"log\"\\n\\t\"sync\"\\n\\n\\t\"github.com/NVIDIA/go-nvml/pkg/nvml\"\\n)\\n\\n// FetchGpuInfo fetches the metrics for all GPU devices\\nfunc FetchAllGpuInfo(ctx context.Context, gpu GpuDeviceManager, count int) (GpuMap, error) {\\n\\n\\tif count == 0 {\\n\\t\\treturn nil, fmt.Errorf(\"no GPU devices found\")\\n\\t}\\n\\n\\tgpuMap := make(GpuMap)\\n\\n\\t// gpuChan := make(chan GpuMap, count)\\n\\t// defer close(gpuChan)\\n\\n\\terrChan := make(chan error, 1) // only need to store one error\\n\\tdefer close(errChan)\\n\\n\\twg := new(sync.WaitGroup)\\n\\twg.Add(count)\\n\\n\\tfor i := 0; i < count; i++ {\\n\\t\\tgo func(i int) {\\n\\t\\t\\tdefer wg.Done()\\n\\n\\t\\t\\t// context done means there has been a cancellation signal\\n\\t\\t\\tselect {\\n\\t\\t\\tcase <-ctx.Done():\\n\\t\\t\\t\\terrChan <- ctx.Err()\\n\\t\\t\\t\\treturn\\n\\t\\t\\tdefault:\\n\\t\\t\\t}\\n\\n\\t\\t\\tdevice, ret := gpu.DeviceGetHandleByIndex(i)\\n\\t\\t\\tif ret != nvml.SUCCESS {\\n\\t\\t\\t\\terrChan <- fmt.Errorf(\"failed to get device handle: %v\", ret)\\n\\t\\t\\t\\treturn\\n\\t\\t\\t}\\n\\n\\t\\t\\tg, err := FetchDeviceMetrics(device)\\n\\t\\t\\tif err != nvml.SUCCESS {\\n\\t\\t\\t\\terrChan <- fmt.Errorf(\"failed to get device info: %v\", err)\\n\\t\\t\\t\\treturn\\n\\t\\t\\t}\\n\\n\\t\\t\\tgpuMap[i] = g\\n\\t\\t}(i)\\n\\t}\\n\\n\\t// main goroutine waits for all goroutines to finish\\n\\twg.Wait()\\n\\n\\tfor i := 0; i < count; i++ {\\n\\t\\tselect {\\n\\t\\tcase <-ctx.Done(): // if context is done, ( cancelled or timeout ) return ctx.err\\n\\t\\t\\treturn nil, ctx.Err()\\n\\t\\tcase err := <-errChan: // if there is an error, return err\\n\\t\\t\\treturn nil, err\\n\\t\\tdefault:\\n\\t\\t\\tlog.Printf(\"Fetched Metrics for GPU %d\\\\n\", i)\\n\\t\\t}\\n\\t}\\n\\n\\treturn gpuMap, nil\\n}', metadata={'source': '/home/rupesh/aqrtr/ai/langchain/proj-101-go/go_repo-01/internal/metrics/fetch_gpu_metrics.go', 'language': <Language.GO: 'go'>})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "db = Chroma.from_documents(texts, OpenAIEmbeddings(disallowed_special=()))\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"mmr\",  # Also test \"similarity\"\n",
    "    search_kwargs={\"k\": 8},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4\")\n",
    "memory = ConversationSummaryMemory(llm=llm, max_memory_length=5, max_memory_turns=5, return_messages=True, memory_key=\"chat_history\")\n",
    "qaChain = ConversationalRetrievalChain.from_llm(llm=llm, memory=memory, retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 20 is greater than number of elements in index 18, updating n_results = 18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Error Handling: The `FetchAllGpuInfo` function could use more comprehensive error handling. Right now, it only returns the first error it encounters, but it could be more informative if it collected all errors and returned them together.\n",
      "\n",
      "2. Parallelism Control: The function spawns a goroutine for each GPU device, which could potentially lead to issues if the count of GPU devices is large. It could be improved by using a worker pool pattern to limit the number of concurrent goroutines.\n",
      "\n",
      "3. Context Usage: The function could make better use of the context passed in. It could pass the context to the `FetchDeviceMetrics` function to allow for cancellation of that function as well.\n",
      "\n",
      "4. Logging: The function could use structured logging to make the logs easier to parse and analyze. Additionally, it could include more information in the logs, such as the specific GPU device that an operation is being performed on.\n",
      "\n",
      "5. Code Duplication: The function has some duplicated code that could be refactored. For example, the error handling and logging code in the goroutines are essentially the same and could be moved into a separate function.\n",
      "\n",
      "6. Result Ordering: The function could ensure that the results are in the same order as the devices. Right now, due to the concurrent nature of the function, the order of the results in the `gpuMap` may not match the order of the devices.\n",
      "\n",
      "7. Testing: The function could be made more testable by breaking it down into smaller, more isolated functions, each with a single responsibility. This would make it easier to write unit tests for each individual component of the function.\n"
     ]
    }
   ],
   "source": [
    "# Ask a question\n",
    "question = \"Can you suggest some improvements for function FetchAllGpuInfo?\"  # Replace with your actual question\n",
    "\n",
    "# Include 'chat_history' key if required by your function\n",
    "formatted_question = {'question': question, 'chat_history': ''}\n",
    "\n",
    "# Invoke the conversation chain with the properly formatted question\n",
    "result = qaChain.invoke(formatted_question)\n",
    "\n",
    "# Fetch and print the answer\n",
    "answer = result['answer']\n",
    "print(answer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models.ollama import ChatOllama\n",
    "llmOllama = ChatOllama(model=\"codellama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without specific context, it can be assumed that the function FetchAllGpuInfo is used to retrieve information about all available GPU (Graphics Processing Unit) on a system. This might include details like the GPU model, memory size, driver version, clock speed, temperature, usage percentage, and other technical specifications. The actual details fetched can vary depending on the programming language and libraries used.\n",
      "{'token_usage': {'completion_tokens': 77, 'prompt_tokens': 18, 'total_tokens': 95}, 'model_name': 'gpt-4', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}\n"
     ]
    }
   ],
   "source": [
    "response_message = llmOllama.invoke(\n",
    "    \"what does function FetchAllGpuInfo do? \"\n",
    ")\n",
    "\n",
    "print(response_message.content)\n",
    "print(response_message.response_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_community.chat_models.ollama import ChatOllama\n",
    "llmOllama = ChatOllama(model=\"codellama\")\n",
    "memoryOllama = ConversationSummaryMemory(llm=llm, max_memory_length=5, max_memory_turns=5, return_messages=True, memory_key=\"chat_history\")\n",
    "ollamaChain = ConversationalRetrievalChain.from_llm(llm=llmOllama, memory=memoryOllama, retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 20 is greater than number of elements in index 18, updating n_results = 18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Here are some test case suggestions for the `FetchAllGpuInfo` function:\n",
      "\n",
      "1. Test that the function returns an error when the context is canceled.\n",
      "2. Test that the function returns an error when the device manager encounters an error while fetching metrics.\n",
      "3. Test that the function returns a slice of GPU information objects with the expected number of elements, given different numbers of GPUs.\n",
      "4. Test that the function returns a slice of GPU information objects with the correct values for each metric. For example, test that the temperature is non-zero and within an acceptable range, the power usage is within an acceptable range, etc.\n",
      "5. Test that the function correctly handles the case where the number of GPUs changes while it's running.\n",
      "6. Test that the function returns an error when it encounters an unsupported GPU model or version.\n",
      "7. Test that the function returns an error when it fails to marshal the GPU information objects into JSON.\n",
      "8. Test that the function correctly handles the case where the device manager encounters an error while fetching a metric for one of the GPUs.\n",
      "9. Test that the function returns the correct number of elements in the slice, even if some of the metrics fail to be fetched.\n",
      "10. Test that the function correctly handles the case where the device manager encounters an error while shutting down the library.\n",
      "\n",
      "Here's an example of how you could write a test for the first point:\n",
      "```go\n",
      "func TestFetchAllGpuInfo(t *testing.T) {\n",
      "\tctx, cancel := context.WithCancel(context.Background())\n",
      "\tdefer cancel()\n",
      "\n",
      "\tdeviceManager := &MockGpuDeviceManager{}\n",
      "\tdeviceManager.On(\"Init\").Return(nvml.SUCCESS)\n",
      "\tdeviceManager.On(\"Shutdown\").Return(nvml.SUCCESS)\n",
      "\n",
      "\tgpuCount := 2\n",
      "\tinfo, err := FetchAllGpuInfo(ctx, deviceManager, gpuCount)\n",
      "\tif err != nil {\n",
      "\t\tt.Fatalf(\"Failed to fetch GPU info: %s\", err)\n",
      "\t}\n",
      "\n",
      "\tcancel()\n",
      "\n",
      "\t// Test that the function returns an error when the context is canceled.\n",
      "\tdeviceManager.AssertExpectations(t)\n",
      "\tassert.ErrorIs(t, ctx.Err(), gpumetrics.ContextCanceledError{})\n",
      "}\n",
      "```\n",
      "In this test, we create a mock device manager that returns `nvml.SUCCESS` for the `Init` and `Shutdown` methods, which means that all GPUs have been successfully initialized and shut down. We then call the `FetchAllGpuInfo` function with a context that will be canceled immediately, as if the program was shutting down. Finally, we test that the function returns an error of type `gpumetrics.ContextCanceledError`, which indicates that the context has been canceled.\n",
      "\n",
      "In summary, this test case verifies that the function correctly handles the cancellation of a context while fetching GPU information from multiple GPUs using the NVIDIA Management Library (NVML).\n"
     ]
    }
   ],
   "source": [
    "# Ask a question\n",
    "question = \"Can you write tests for function FetchAllGpuInfo?\"  # Replace with your actual question\n",
    "\n",
    "# Include 'chat_history' key if required by your function\n",
    "formatted_question = {'question': question, 'chat_history': ''}\n",
    "\n",
    "# Invoke the conversation chain with the properly formatted question\n",
    "result = ollamaChain.invoke(formatted_question)\n",
    "\n",
    "# Fetch and print the answer\n",
    "answer = result['answer']\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ask a question\n",
    "question = \"Can you write read me for the nvidia-server repo?\"  # Replace with your actual question\n",
    "\n",
    "# Include 'chat_history' key if required by your function\n",
    "formatted_question = {'question': question, 'chat_history': ''}\n",
    "\n",
    "# Invoke the conversation chain with the properly formatted question\n",
    "result = ollamaChain.invoke(formatted_question)\n",
    "\n",
    "# Fetch and print the answer\n",
    "answer = result['answer']\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
